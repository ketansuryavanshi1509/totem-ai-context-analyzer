
# Totem â€” AI Context Analyzer (Prototype)

Totem is an AI-driven tool that evaluates the **quality**, **completeness**, and **semantic coverage** of an AI-generated answer based on a userâ€™s original question.  
It detects **missing topics**, assigns a **quality score**, generates **follow-up prompts**, and produces **localized meta-guidance** on how to improve the answer.

It supports **multiple languages** (English, Hindi, Marathi, Spanish) and uses multilingual embeddings for semantic comparison.

---

## ğŸš€ Features

### ğŸ” 1. Semantic Coverage Detection
- Splits the user prompt into meaningful sentences  
- Embeds each sentence using multilingual MiniLM  
- Scores how well the AIâ€™s answer covers each part of the question

### ğŸ“Š 2. Quality Score (0â€“10)
- Based on semantic similarity  
- Penalizes answers that are:
  - Too short  
  - Off-topic  
  - Missing parts of the question

### âš ï¸ 3. Missing Topic Extraction
Identifies which parts of the query are **not answered at all**.

### ğŸ’¬ 4. Follow-up Prompt Generation
Generates intelligent follow-up questions to get a better answer:
- Ask for examples
- Ask for clarity
- Ask for missing topics
- Ask for limitations / steps

### ğŸ§  5. Improved Answer (Meta-Guidance)
A non-generative module that:
- Analyzes what is missing  
- Suggests what to add  
- Gives improvement structure  
- Does **NOT** hallucinate or fabricate facts  
(Preferred for evaluation, safe & deterministic.)

### ğŸŒ 6. Multilingual Support
Works in:
- English (`en`)
- Hindi (`hi`)
- Marathi (`mr`)
- Spanish (`es`)

Localized output for:
- Summary  
- Follow-up prompts  
- Suggestions  
- Improvement guidance  

### ğŸ–¥ 7. Streamlit UI
A simple, clean UI for testing:

- Paste user prompt  
- Paste AI response  
- Choose language  
- View score, missing topics, suggestions, guidance

---

## ğŸ“ Project Structure

```

totem-assignment/
â”œâ”€ app/
â”‚  â”œâ”€ main.py              # FastAPI backend
â”‚  â”œâ”€ analyzer.py          # Core logic (scoring, embeddings, multilingual output)
â”‚  â”œâ”€ models.py            # Request/Response schemas
â”‚  â””â”€ utils/
â”‚     â”œâ”€ langutils.py      # Language detection + templates
â”‚     â””â”€ textutils.py      # Splitting + cleaning
â”‚
â”œâ”€ ui/
â”‚  â””â”€ streamlit_app.py     # Frontend UI
â”‚
â”œâ”€ samples/
â”‚  â””â”€ sample_input.json
â”‚
â”œâ”€ requirements.txt
â””â”€ README.md


## ğŸ”§ Tech Stack

- **FastAPI** (backend)
- **Streamlit** (frontend)
- **Sentence-Transformers**  
  - Model: `paraphrase-multilingual-MiniLM-L12-v2`
- **PyTorch**
- **langdetect**
- **NumPy**

No training required.  
All models run in inference mode.

---

## ğŸ›  Installation & Setup

### 1ï¸âƒ£ Create Virtual Environment

```bash
python -m venv venv
````

Activate it:

**Windows:**

```bash
venv\Scripts\activate
```

**macOS / Linux:**

```bash
source venv/bin/activate
```

---

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

### 3ï¸âƒ£ Run Backend (FastAPI)

```bash
uvicorn app.main:app --reload --port 8000
```

API Docs:
ğŸ‘‰ [http://localhost:8000/docs](http://localhost:8000/docs)

---

### 4ï¸âƒ£ Run Frontend (Streamlit)

In a new terminal:

```bash
streamlit run ui/streamlit_app.py
```

UI opens at:
ğŸ‘‰ [http://localhost:8501](http://localhost:8501)

---

## ğŸ§  How It Works (Logic Flow)

### **1. Sentence Splitting**

User prompt is split and cleaned.

### **2. Instruction Filtering**

Generic instructions like:

* "Explain with example"
* "Give example"
* "Explain step by step"

are **ignored** in scoring.

### **3. Embedding**

The system generates embeddings for:

* each user sentence
* each AI answer sentence

### **4. Semantic Similarity**

Cosine similarity is computed.

If similarity < threshold â†’ marked as **missing topic**.

### **5. Quality Score**

Average similarity â†’ scaled to 0â€“10.

### **6. Multilingual Output**

Localized strings generated using predefined templates.

### **7. Follow-up Prompts**

Generated based on what's missing.

### **8. Improved Answer (Meta-Guidance)**

Outlines how to fix the shortcomings without generating new factual content.

---

## ğŸ§ª Example Tests

### ğŸ”´ **Bad Answer Example**

**User Prompt:**

> What is machine learning? Explain with an example.

**AI Response:**

> Machine learning is a dance.

â¡ Score: **Very low**
â¡ Missing topic: full explanation, example
â¡ Follow-up prompts suggested
â¡ Guidance given

---

### ğŸŸ¢ **Good Answer Example**

**User Prompt:**

> What is overfitting in machine learning? Explain with an example.

**AI Response:**

> Overfitting happens when...

â¡ Score: **High**
â¡ Minimal missing topics
â¡ Better guidance

---

### ğŸŒ **Hindi Example**

Prompt:

```
à¤•à¥ƒà¤¤à¥à¤°à¤¿à¤® à¤¬à¥à¤¦à¥à¤§à¤¿à¤®à¤¤à¥à¤¤à¤¾ à¤•à¥à¤¯à¤¾ à¤¹à¥ˆ? à¤‡à¤¸à¤•à¥‡ à¤ªà¥à¤°à¤•à¤¾à¤° à¤”à¤° à¤‰à¤ªà¤¯à¥‹à¤— à¤­à¥€ à¤¬à¤¤à¤¾à¤à¤‚à¥¤
```

â¡ System detects `hi`
â¡ Output localized in Hindi

---

## ğŸ“Œ Limitations

* Does not check factual correctness
* Does not generate new answers (only meta-feedback)
* Multilingual output uses templates, not full translation
* Large questions with many subtopics may need fine-tuning

---

## ğŸš€ Future Improvements

* Add small generative model (optional)
* Add deeper translation models
* Add context history support
* Add weighted coverage scoring
* UI improvements

---

## ğŸ‘¤ Author

ketan suryavanshi
AI Developer | Backend Developer

---

## ğŸ“œ License

This project is for educational and assignment purposes.

